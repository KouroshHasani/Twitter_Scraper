{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "5c2ead3f-9def-4da7-add6-31eb44d6811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import DATA_DIR\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "from hashlib import sha256\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2351fa5-218d-4e6a-806d-23a8d8bb3bfe",
   "metadata": {},
   "source": [
    "##### \n",
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "947476f8-6a77-4902-9472-db04f65dfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_name_list = ['روغن موتور.csv',\n",
    "                       'تعویض روغنی.csv',\n",
    "                       'روغن ماشین.csv',\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "3f236fcd-4a4b-4b3c-9330-feda6cfad3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / 'data' / data_file_name_list[0])\n",
    "\n",
    "for path in data_file_name_list[1:]:\n",
    "    df = pd.concat([df, pd.read_csv(DATA_DIR / 'data' / path)])\n",
    "\n",
    "df = df.drop_duplicates(subset = \"Tweet_ID\")\n",
    "\n",
    "_ = pd.read_csv(DATA_DIR / 'Input/Themes.csv')\n",
    "themes = {_.iloc[i, 1]:[sha256(_.iloc[i, 0].encode()).hexdigest()[:18], _.iloc[i, 0].replace(\" \", \"_\")] for i in range(len(_))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "f3473967-dd8c-461c-9427-1340c73f1b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in list(df[df.Text.str.contains('عوض') * df.Text.str.contains('تومن')].Text):\n",
    "#     print(i)\n",
    "#     print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "6288057c-67d8-47bb-8d34-8cda3ba757fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counter(\" \".join(df[df.Text.str.contains('مکانیک')].Text).split()).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "241644be-6b4b-4020-8084-27791fedc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_id = ['nilaka',\n",
    "             'مارلیک',\n",
    "             'baryar',\n",
    "             'arazoilco',\n",
    "            ]\n",
    "\n",
    "delete_tweet = ['گوجه',\n",
    "                'مایع',\n",
    "                'خوراکی',\n",
    "                'گوشت',\n",
    "                'مرغ',\n",
    "                'جمهوری',\n",
    "                'غذا',\n",
    "                ['گرون', 'تقلبی'],\n",
    "                ['گران', 'تقلبی'],\n",
    "                ['گرون', 'فیک'],\n",
    "                ['گران', 'فیک'],\n",
    "               ]\n",
    "\n",
    "for id_ in delete_id:\n",
    "    df = df[~df.User_ID.str.contains(id_)]\n",
    "    \n",
    "for item in delete_tweet:\n",
    "    if isinstance(item, str):\n",
    "        df = df[~df.Text.str.contains(item)]\n",
    "\n",
    "    elif isinstance(item, list):\n",
    "        rem = df.Text.str.contains(item[0])\n",
    "        for j in item[1:]:\n",
    "            rem = rem & df.Text.str.contains(j)\n",
    "            \n",
    "        df = df[~rem]\n",
    "    \n",
    "tweet_texts = ' Oadf6dsaf8dfa '.join(list(df.Text))\n",
    "\n",
    "tweet_texts2 = copy.deepcopy(tweet_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b81dd-eb0d-4490-b5c2-b92e6ec3a571",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### \n",
    "### Detect Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "70dabc63-c147-4d14-9e99-87b95e73bfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Thenes\n",
    "\n",
    "for word_, theme_ in themes.items():\n",
    "    tweet_texts = tweet_texts.replace(word_, theme_[0])\n",
    "\n",
    "for theme_ in themes.values():\n",
    "    tweet_texts = tweet_texts.replace(theme_[0], theme_[1])\n",
    "\n",
    "filt = [*[item[1] for item in list(themes.values())], 'Oadf6dsaf8dfa']\n",
    "\n",
    "_ = list(filter(lambda x: x in filt, tweet_texts.split()))\n",
    "tweet_list = [list(group) for k, group in groupby(_, lambda x: x == \"Oadf6dsaf8dfa\") if not k]\n",
    "\n",
    "_ = [(item[0], item[0].replace(\"_\", \" \"), item[1]) for item in Counter(_).most_common()[1:]]\n",
    "df_output_nodes = pd.DataFrame(_, columns=['Id', 'Label', 'Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "569963aa-09da-42fb-a057-95b67395dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = defaultdict(int)\n",
    "\n",
    "for tweet in tweet_list:\n",
    "    set_ = set()\n",
    "    if len(tweet) <=1:\n",
    "        continue\n",
    "    \n",
    "    for i in range(len(tweet)):\n",
    "        for j in range(i+1, len(tweet)):\n",
    "            if tweet[i] != tweet[j]:\n",
    "                if (tweet[i], tweet[j]) in set_:\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    edges[(tweet[i], tweet[j])] += 1\n",
    "                    set_.add((tweet[i], tweet[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "e20f7bc2-db72-496b-9f28-f655a5381cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "\n",
    "filt = [*list(themes.keys()), 'Oadf6dsaf8dfa']\n",
    "_ = list(filter(lambda x: x in filt, tweet_texts2.split()))\n",
    "tweet_list_word = [list(group) for k, group in groupby(_, lambda x: x == \"Oadf6dsaf8dfa\") if not k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "5fecf314-5792-41c7-9d9f-bd29a7a3dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_word = defaultdict(int)\n",
    "\n",
    "for tweet in tweet_list_word:\n",
    "    set_ = set()\n",
    "    if len(tweet) <=1:\n",
    "        continue\n",
    "    \n",
    "    for i in range(len(tweet)):\n",
    "        for j in range(i+1, len(tweet)):\n",
    "            if tweet[i] != tweet[j]:\n",
    "                if (tweet[i], tweet[j]) in set_:\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    edges_word[(tweet[i], tweet[j])] += 1\n",
    "                    set_.add((tweet[i], tweet[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137f603-8163-4f8e-ab41-c2fba5e59a43",
   "metadata": {},
   "source": [
    "###### \n",
    "### Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "811f83eb-2a56-47af-b6d0-b770fc123b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = []\n",
    "tweet_list_word = []\n",
    "\n",
    "for key, value in edges.items():\n",
    "    tweet_list.append((*key, value))\n",
    "    \n",
    "for key, value in edges_word.items():\n",
    "    tweet_list_word.append((*key, value))\n",
    "\n",
    "df_output_edges = pd.DataFrame(tweet_list, columns=['Source', 'Target', 'Weight'])\n",
    "df_output_edges.to_csv(DATA_DIR / 'data/Gephi/edges.csv', index=False, encoding=\"utf-8-sig\")\n",
    "df_output_nodes.to_csv(DATA_DIR / 'data/Gephi/nodes.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_output_edges_word = pd.DataFrame(tweet_list_word, columns=['Source', 'Target', 'Weight'])\n",
    "df_output_edges_word = df_output_edges_word.sort_values('Weight', ascending=False)\n",
    "df_output_edges_word.to_csv(DATA_DIR / 'data/Gephi/edges_word.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c989f45-8137-43f9-baa4-2dec0cd82cd5",
   "metadata": {},
   "source": [
    "###### \n",
    "### Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "15dda0df-b0d5-40a2-b04a-265a554c33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text: str, stopwords: list):\n",
    "    \"\"\"\n",
    "    :param text: text you want to delete stopwords from dat\n",
    "    :param stopwords: list of stopwords\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = filter(lambda word: word not in stopwords, tokens)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "8b2a4dcb-1303-4fd5-abaf-6ca7fc96ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / 'Input/persian_stop_words.txt') as fp:\n",
    "    stopwords = fp.read()\n",
    "\n",
    "stopwords = stopwords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "8db0bbc9-27d2-42ac-afc4-267bf73fd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = list(df.Text)\n",
    "list_ = [str(text) for text in list_]\n",
    "text = ' '.join(list_)\n",
    "text = remove_stopwords(text, stopwords)\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "ff65aa70-7252-4933-aebd-d78fc03ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "df = pd.DataFrame(Counter(text).most_common(), columns=['word', 'count'])\n",
    "df.to_csv(DATA_DIR / 'Input/most_common_words.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34474268-3d31-4eb2-9a8e-d42b6071ff7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
