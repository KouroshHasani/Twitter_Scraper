{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9a68ed-60fb-452b-b0ac-44f84697c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from hashlib import sha256\n",
    "import os.path\n",
    "from src import DATA_DIR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1771c969-1f18-4a73-981f-9813f3f78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter_scraper:\n",
    "    def __init__(self):\n",
    "        options = EdgeOptions()\n",
    "        options.getchromium = True\n",
    "        self.driver = Edge('/mnt/f/msedgedriver.exe', options=options)\n",
    "        \n",
    "        sleep(2)\n",
    "        self.driver.get('https://twitter.com/explore')\n",
    "        sleep(3)\n",
    "    \n",
    "    \n",
    "    def get_tweet_data(self, search_query:dict, max_tweet=None):\n",
    "        \n",
    "        ''' \n",
    "        *** Extract tweets ***\n",
    "        \n",
    "                :param search_query: (dict) Gets search query. The dictionary can have following keys:\n",
    "                    - include: (list) Words that must be in tweet text\n",
    "                    - include_operator: ('AND', 'OR' - default='And')\n",
    "                    - not_include: (list) Words that shouldn't be in tweet text\n",
    "                    \n",
    "                    - since: (str, samp = “2015-12-21”) Containing tweets that sent since a specific date\n",
    "                    - until: (str, samp = “2022-12-21”) Containing tweets that sent until a specific date\n",
    "                    - from: (list) Filters tweets that sent from specific Twitter accounts\n",
    "                    - to: (list) Filters tweets that sent in reply to specific Twitter accounts\n",
    "                    - mentioning: (list) Containing tweets that mentioning specific Twitter accounts\n",
    "                    - question: ('True', 'False') If True, gets all tweet, that were recognized by twitter AI as a question\n",
    "                \n",
    "                :param max_tweet= (int) Ahen reachs to the number, running will stop\n",
    "                \n",
    "        '''\n",
    "        exprot_file_name = '_'.join(search_query['include'])\n",
    "        serach_text = ''\n",
    "        \n",
    "        if 'from' in search_query.keys():\n",
    "            serach_text += 'from:' + ' OR from:'.join(search_query['from'])\n",
    "            \n",
    "        if 'to' in search_query.keys():\n",
    "            serach_text += ' to:' + ' OR to:'.join(search_query['to'])\n",
    "        \n",
    "        if 'since' in search_query.keys():\n",
    "            serach_text += f\" since:{search_query['since']}\"\n",
    "            \n",
    "        if 'until' in search_query.keys():\n",
    "            serach_text += f\" until:{search_query['until']}\"\n",
    "        \n",
    "        if 'include_operator' in search_query.keys():\n",
    "            if search_query['include_operator'] == 'OR':\n",
    "                serach_text += ' ' + ' OR '.join(search_query['include'])\n",
    "                \n",
    "            else:\n",
    "                serach_text += ' ' + ' '.join(search_query['include'])\n",
    "        \n",
    "        else: \n",
    "            serach_text = ' '.join(search_query['include'])\n",
    "                                   \n",
    "        if 'not_include' in search_query.keys():\n",
    "            serach_text = ' -'.join([serach_text, *search_query['not_include']])\n",
    "        \n",
    "        if 'mentioning' in search_query.keys():\n",
    "            serach_text = ' @'.join([serach_text, *search_query['mentioning']])\n",
    "        \n",
    "        \n",
    "        sleep(6)\n",
    "        search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "        search.send_keys(Keys.CONTROL + \"a\")\n",
    "        search.send_keys(Keys.DELETE)\n",
    "        search.send_keys(serach_text)\n",
    "        search.send_keys(Keys.RETURN)\n",
    "\n",
    "        sleep(2)\n",
    "        self.driver.find_element('link text', 'Latest').click()\n",
    "\n",
    "        unique_tweet_ids = set()\n",
    "        last_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "        scrolling = True\n",
    "        list_ = []\n",
    "        progress = 1\n",
    "        \n",
    "        while scrolling:\n",
    "            \n",
    "            sleep(2)\n",
    "            tweets = self.driver.find_elements('xpath', '//article[@data-testid=\"tweet\"]')  \n",
    "\n",
    "            for tweet in tweets[-min(24, len(tweets)-1):]:\n",
    "                try:\n",
    "                    reply_to, hashtag = str(), str()\n",
    "                    text = tweet.find_element('xpath', './/div[@data-testid=\"tweetText\"]').text\n",
    "\n",
    "                    if 'Replying to \\n@' in tweet.text:\n",
    "                        reply_to = tweet.find_element('xpath', './/div[@class=\"css-901oao r-1bwzh9t r-37j5jr r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"]').text.split()\n",
    "                        reply_to = [id_ for id_ in reply_to if id_[0] == '@']\n",
    "\n",
    "                    if '#' in text:\n",
    "                        words = text.split()\n",
    "                        hashtag = [word for word in words if word[0] == '#']\n",
    "\n",
    "                    reply = tweet.find_element('xpath', './/div[@data-testid=\"reply\"]').text\n",
    "                    retweet = tweet.find_element('xpath', './/div[@data-testid=\"retweet\"]').text\n",
    "                    like = tweet.find_element('xpath', './/div[@data-testid=\"like\"]').text\n",
    "\n",
    "                    header = tweet.find_element('xpath', './/div[@data-testid=\"User-Names\"]')\n",
    "                    _ = header.find_element('xpath', './/time').get_attribute('datetime')\n",
    "                    datetime = dt.datetime.strptime(_, '%Y-%m-%dT%H:%M:%S.000Z') + timedelta(hours=4.5)\n",
    "\n",
    "                    info = header.text.split('@')\n",
    "                    username = info[0].strip('\\n')\n",
    "                    user_id = '@' + info[1].split()[0]\n",
    "\n",
    "                    tweet_id = sha256(\"\".join([user_id, str(datetime), text]).encode()).hexdigest()\n",
    "                    \n",
    "                    if tweet_id not in unique_tweet_ids:\n",
    "                        sys.stdout.write('\\r')\n",
    "                        sys.stdout.write(\"[%-0s] %d\" % ('', progress))\n",
    "                        sys.stdout.flush()\n",
    "                        progress += 1\n",
    "                        \n",
    "                        list_.append([tweet_id, username, user_id, datetime, text, reply_to, hashtag, reply, retweet, like])\n",
    "                        unique_tweet_ids.add(tweet_id)\n",
    "                        \n",
    "                        if len(list_) >= 100 or progress > max_tweet:\n",
    "                            df = pd.DataFrame(list_, columns=['Tweet_ID', 'Username',\n",
    "                                                              'User_ID', 'DateTime',\n",
    "                                                              'Text', 'Reply_to',\n",
    "                                                              'Hashtag', 'Reply',\n",
    "                                                              'Retweet', 'Like',\n",
    "                                                             ])\n",
    "                            \n",
    "                            if not os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "                                df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\")\n",
    "                                \n",
    "                            else:\n",
    "                                df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", mode='a', header=False)\n",
    "                                \n",
    "                            list_ = []\n",
    "                            \n",
    "                        if progress > max_tweet:\n",
    "                            scrolling = False\n",
    "                            break\n",
    "                            \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Scrolling\n",
    "            scroll_attemp = 0\n",
    "            \n",
    "            while True:\n",
    "                self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                sleep(4)\n",
    "                current_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                if current_positon == last_positon:\n",
    "                    scroll_attemp += 1\n",
    "                    \n",
    "                    if scroll_attemp >= 4:\n",
    "                        if len(list_) > 0:\n",
    "                            df = pd.DataFrame(list_, columns=['Tweet_ID', 'Username',\n",
    "                                                              'User_ID', 'DateTime',\n",
    "                                                              'Text', 'Reply_to',\n",
    "                                                              'Hashtag', 'Reply',\n",
    "                                                              'Retweet', 'Like',\n",
    "                                                             ])\n",
    "                            if not os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "                                df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\")\n",
    "                                \n",
    "                            else:\n",
    "                                df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", mode='a', header=False)\n",
    "                            \n",
    "                        scrolling = False\n",
    "                        break\n",
    "                    \n",
    "                    else:\n",
    "                        sleep(3)\n",
    "                \n",
    "                else:\n",
    "                    last_positon = current_positon\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1e0e-cfa9-430b-a2fd-cede581c0193",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177e320f-b9ca-4b1b-8385-4566fabdf971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet = Twitter_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa44e35-2338-465e-8c5e-a4585dca52a5",
   "metadata": {},
   "source": [
    "##### 01_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366307f5-e692-4399-a694-b32f517725a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] 12"
     ]
    }
   ],
   "source": [
    "tweet.get_tweet_data(\n",
    "                    search_query={'include': ['خلاقیت', 'بادکوبه'],\n",
    "                                  'include_operator': 'AND',\n",
    "                                  'until': '2022-07-06',                                  \n",
    "                                 },\n",
    "                    max_tweet=None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cb640-eb6e-4826-b1d3-578138cc4d1c",
   "metadata": {},
   "source": [
    "##### 02_complete version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57ad979e-67f5-49d2-8f43-34d9b4cb09c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] 1"
     ]
    }
   ],
   "source": [
    "tweet.get_tweet_data(\n",
    "                    search_query={'include': ['space', 'earth'],\n",
    "                                  'include_operator': 'AND',\n",
    "                                  'not_include': ['satellite'],\n",
    "                                  'since': '2018-01-01',\n",
    "                                  'until': '2022-01-01',\n",
    "                                  'from': ['tonyandthesun'],\n",
    "                                  'to': ['NASA', 'SpaceX'],\n",
    "                                  'mentioning': ['SpaceX']\n",
    "                                  \n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f1deb-2af1-48fc-aada-a887770442fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
