{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9a68ed-60fb-452b-b0ac-44f84697c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from hashlib import sha256\n",
    "import os.path\n",
    "from src import DATA_DIR\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1771c969-1f18-4a73-981f-9813f3f78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter_scraper:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        \n",
    "\n",
    "    def save_file(self, list_, exprot_file_name):\n",
    "        df = pd.DataFrame(list_, columns=['Tweet_ID', 'Username',\n",
    "                                          'User_ID', 'DateTime',\n",
    "                                          'Text', 'Reply_to',\n",
    "                                          'Hashtag', 'Reply',\n",
    "                                          'Retweet', 'Like',\n",
    "                                         ])\n",
    "\n",
    "        if not os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "        else:\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", mode='a', header=False, index=False)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def create_search_query(self, search_query:dict):\n",
    "        \n",
    "        '''\n",
    "        *** Create Search Query ***\n",
    "        :param search_query: (dict or path) Gets search query. If search_querys type is dictionary, It can have following keys:\n",
    "            - include: (list) Words that must be in tweet text. Each element is a list. This method will combine different elements with 'AND'\n",
    "              operator and each element words with 'OR' operator.\n",
    "              \n",
    "            - not_include: (list) Words that shouldn't be in tweet text. Each element is a list. This method will combine different elements\n",
    "              with 'AND' operator and each element words with 'OR' operator.\n",
    "\n",
    "            - from: (list) Filters tweets that sent from specific Twitter accounts\n",
    "\n",
    "            - to: (list) Filters tweets that sent in reply to specific Twitter accounts\n",
    "\n",
    "            - since: (str, samp = “2015-12-21”) Containing tweets that sent since a specific date\n",
    "\n",
    "            - until: (str, samp = “2022-12-21”) Containing tweets that sent until a specific date\n",
    "\n",
    "            - mentioning: (list) Containing tweets that mentioning specific Twitter accounts\n",
    "\n",
    "            - question: ('True', 'False') If True, gets all tweet, that were recognized by twitter AI as a question\n",
    "                \n",
    "        '''\n",
    "\n",
    "        search_text = ''\n",
    "        _ = []\n",
    "\n",
    "        for elem in search_query['include']:\n",
    "            _.append(\"((\" + \") OR (\".join(elem) + \"))\")\n",
    "        search_text = \"((\" + \") AND (\".join(_) + \"))\"\n",
    "\n",
    "        if 'not_include' in search_query.keys():\n",
    "            _ = []\n",
    "            for elem in search_query['not_include']:\n",
    "                _.append(\"((\" + \") AND (\".join(elem) + \"))\")\n",
    "            search_text += \" -((\" + \") OR (\".join(_) + \"))\"\n",
    "\n",
    "        if 'from' in search_query.keys():\n",
    "            search_text += ' (from:' + ' OR from:'.join(search_query['from']) + ')'\n",
    "\n",
    "        if 'to' in search_query.keys():\n",
    "            search_text += ' (to:' + ' OR to:'.join(search_query['to']) + ')'\n",
    "\n",
    "        if 'lang' in search_query.keys():\n",
    "            search_text += ' lang:' + search_query['lang']\n",
    "\n",
    "        if 'since' in search_query.keys():\n",
    "            search_text += f\" since:{search_query['since']}\"\n",
    "\n",
    "        if 'until' in search_query.keys():\n",
    "            search_text += f\" until:{search_query['until']}\"\n",
    "\n",
    "        if 'mentioning' in search_query.keys():\n",
    "            search_text += ' (@' + ' @'.join([search_query['mentioning']]) + ')'\n",
    "            \n",
    "        return search_text\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_tweet_data(self, exprot_file_name, search_query, max_tweet=None):\n",
    "        \n",
    "        ''' \n",
    "        *** Extract tweets ***\n",
    "        :param search_query: (dict or path)\n",
    "            - If search_query is a path, it should redirect to a .txt file that contains search query you want\n",
    "            - If search_query is a dict. it should defined like 'create_search_query' method.\n",
    "        \n",
    "        :param max_tweet: (int) Ahen reachs to the number, running will stop\n",
    "        '''\n",
    "        \n",
    "        sleep(1)\n",
    "        self.driver.get('https://twitter.com/explore')\n",
    "        sleep(4)\n",
    "        \n",
    "        \n",
    "        # Read Unique IDs\n",
    "        unique_tweet_ids = set()\n",
    "        if os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "            df = pd.read_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\")\n",
    "            unique_tweet_ids = set(df.Tweet_ID)\n",
    "        \n",
    "        \n",
    "        # Create Search Query\n",
    "        search_list = []\n",
    "        if isinstance(search_query, dict):\n",
    "            search_list.append(self.create_search_query(search_query))\n",
    "        else:\n",
    "            with open(search_query) as fp:\n",
    "                search_list = fp.read().split('\\n')\n",
    "        \n",
    "        progress = 1\n",
    "        for exp_ind, search_text in enumerate(search_list):\n",
    "            # Enter Search Query in Twitter search box\n",
    "            sleep(4)\n",
    "            search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "            search.send_keys(Keys.CONTROL + \"a\")\n",
    "            search.send_keys(Keys.DELETE)\n",
    "            search.send_keys(search_text)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            # Go to 'Latest' tweets\n",
    "            sleep(10)\n",
    "            self.driver.find_element('link text', 'Latest').click()\n",
    "\n",
    "            # Scraping\n",
    "            last_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "            scrolling = True\n",
    "            list_ = []\n",
    "\n",
    "            while scrolling:\n",
    "                sleep(2)\n",
    "                tweets = self.driver.find_elements('xpath', '//article[@data-testid=\"tweet\"]')  \n",
    "\n",
    "                for tweet in tweets[-min(24, len(tweets)-1):]:\n",
    "                    try:\n",
    "                        reply_to, hashtag = str(), str()\n",
    "                        text = tweet.find_element('xpath', './/div[@data-testid=\"tweetText\"]').text\n",
    "\n",
    "                        if 'Replying to \\n@' in tweet.text:\n",
    "                            reply_to = tweet.find_element(\n",
    "                                'xpath', './/div[@class=\"css-901oao r-1bwzh9t r-37j5jr r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"]'\n",
    "                            ).text.split()\n",
    "                            reply_to = [id_ for id_ in reply_to if id_[0] == '@']\n",
    "\n",
    "                        if '#' in text:\n",
    "                            words = text.split()\n",
    "                            hashtag = [word for word in words if word[0] == '#']\n",
    "\n",
    "                        reply = tweet.find_element('xpath', './/div[@data-testid=\"reply\"]').text\n",
    "                        retweet = tweet.find_element('xpath', './/div[@data-testid=\"retweet\"]').text\n",
    "                        like = tweet.find_element('xpath', './/div[@data-testid=\"like\"]').text\n",
    "\n",
    "                        header = tweet.find_element('xpath', './/div[@data-testid=\"User-Names\"]')\n",
    "                        _ = header.find_element('xpath', './/time').get_attribute('datetime')\n",
    "                        datetime = dt.datetime.strptime(_, '%Y-%m-%dT%H:%M:%S.000Z') + timedelta(hours=4.5)\n",
    "\n",
    "                        info = header.text.split('@')\n",
    "                        username = info[0].strip('\\n')\n",
    "                        user_id = '@' + info[1].split()[0]\n",
    "\n",
    "                        tweet_id = sha256(\"\".join([user_id, str(datetime), text]).encode()).hexdigest()\n",
    "\n",
    "\n",
    "                        if tweet_id not in unique_tweet_ids:\n",
    "                            sys.stdout.write('\\r')\n",
    "                            sys.stdout.write(f\"Saved: {progress - progress % 100}       \" +\n",
    "                                             f\"Current_State: [ Search_Query: {exp_ind + 1} from {len(search_list)}    \" +\n",
    "                                             f\"Tweet: {progress}    \" +\n",
    "                                             f\"Date: {str(datetime)[:7]} ] {' '*20}‌\")\n",
    "                            \n",
    "                            sys.stdout.flush()\n",
    "                            progress += 1\n",
    "\n",
    "                            list_.append([tweet_id, username, user_id, datetime, text, reply_to, hashtag, reply, retweet, like])\n",
    "                            unique_tweet_ids.add(tweet_id)\n",
    "\n",
    "                            if len(list_) >= 100 or progress > max_tweet:\n",
    "                                self.save_file(list_, f'{exprot_file_name}_{exp_ind}')\n",
    "                                list_ = []\n",
    "\n",
    "                            if progress > max_tweet:\n",
    "                                scrolling = False\n",
    "                                break\n",
    "\n",
    "                            if progress % 500 == 0:\n",
    "                                search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "                                search.send_keys(Keys.CONTROL + \"a\")\n",
    "                                search.send_keys(Keys.DELETE)\n",
    "                                search.send_keys(search_text + str(datetime)[:10])\n",
    "                                search.send_keys(Keys.RETURN)\n",
    "                                sleep(10)\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Scrolling\n",
    "                scroll_attemp = 0\n",
    "\n",
    "                while True:\n",
    "                    self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                    sleep(4)\n",
    "                    current_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                    if current_positon == last_positon:\n",
    "                        scroll_attemp += 1  \n",
    "                        if scroll_attemp >= 5:\n",
    "                            if len(list_) > 0:\n",
    "                                self.save_file(list_, f'{exprot_file_name}_{exp_ind}')\n",
    "                                list_ = []\n",
    "\n",
    "                            scrolling = False\n",
    "                            break\n",
    "                        else:\n",
    "                            sleep(5)\n",
    "                    else:\n",
    "                        last_positon = current_positon\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1e0e-cfa9-430b-a2fd-cede581c0193",
   "metadata": {},
   "source": [
    "### \n",
    "## Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe16a086-0457-40d2-8094-b54636c48456",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "options.add_argument(\"-inprivate\")\n",
    "options.binary_location = r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "\n",
    "driver = Edge('/mnt/f/msedgedriver.exe', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "177e320f-b9ca-4b1b-8385-4566fabdf971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet = Twitter_scraper(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa44e35-2338-465e-8c5e-a4585dca52a5",
   "metadata": {},
   "source": [
    "##### 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "366307f5-e692-4399-a694-b32f517725a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 300       Current_State: [ Search_Query: 1 from 1    Tweet: 318    Date: 2022-07 ]                     ‌"
     ]
    }
   ],
   "source": [
    "tweet.get_tweet_data(\n",
    "    search_query={'include': [['بورس', 'شاخص کل', 'فرابوس'\n",
    "                              ],\n",
    "                              ['شاخص کل', 'شاخص قیمت'\n",
    "                              ],\n",
    "                             ],\n",
    "                  \n",
    "                  'not_include': [['ایران',\n",
    "                                  ],\n",
    "                                  ['آخوند'\n",
    "                                  ],\n",
    "                                  ['جمهوری'\n",
    "                                  ]\n",
    "                                 ],\n",
    "                  \n",
    "                  'lang': 'fa',\n",
    "                  \n",
    "                 },\n",
    "    max_tweet=None,\n",
    "    exprot_file_name='All_In_One')\n",
    "\n",
    "\n",
    "# tweet.get_tweet_data(\n",
    "#     search_query=DATA_DIR / 'data/Input/search_query.txt'\n",
    "#     max_tweet=None,\n",
    "#     exprot_file_name='All_In_One')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c411e7a-2f5f-4af1-8d8c-8e6cc59c9636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
