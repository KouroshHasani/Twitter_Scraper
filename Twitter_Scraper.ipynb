{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9a68ed-60fb-452b-b0ac-44f84697c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from hashlib import sha256\n",
    "import os.path\n",
    "from src import DATA_DIR\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from src import DATA_DIR\n",
    "import pandas as pd\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "from hashlib import sha256\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1771c969-1f18-4a73-981f-9813f3f78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter_scraper:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        \n",
    "\n",
    "    def save_file(self, list_, exprot_file_name):\n",
    "        df = pd.DataFrame(list_, columns=['Search_Query', 'Tweet_ID',\n",
    "                                          'Username', 'User_ID',\n",
    "                                          'DateTime', 'Text',\n",
    "                                          'Reply_to', 'Hashtag',\n",
    "                                          'Reply', 'Retweet',\n",
    "                                          'Like',\n",
    "                                         ])\n",
    "\n",
    "        if not os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "        else:\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", mode='a', header=False, index=False)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def create_search_query(self, search_query:dict):\n",
    "        \n",
    "        '''\n",
    "        *** Create Search Query ***\n",
    "        :param search_query: (dict or path) Gets search query. If search_querys type is dictionary, It can have following keys:\n",
    "            - include: (list) Words that must be in tweet text. Each element is a list. This method will combine different elements with 'AND'\n",
    "              operator and each element words with 'OR' operator.\n",
    "              \n",
    "            - not_include: (list) Words that shouldn't be in tweet text. Each element is a list. This method will combine different elements\n",
    "              with 'AND' operator and each element words with 'OR' operator.\n",
    "\n",
    "            - from: (list) Filters tweets that sent from specific Twitter accounts\n",
    "\n",
    "            - to: (list) Filters tweets that sent in reply to specific Twitter accounts\n",
    "\n",
    "            - since: (str, samp = “2015-12-21”) Containing tweets that sent since a specific date\n",
    "\n",
    "            - until: (str, samp = “2022-12-21”) Containing tweets that sent until a specific date\n",
    "\n",
    "            - mentioning: (list) Containing tweets that mentioning specific Twitter accounts\n",
    "\n",
    "            - question: ('True', 'False') If True, gets all tweet, that were recognized by twitter AI as a question\n",
    "                \n",
    "        '''\n",
    "\n",
    "        search_text = ''\n",
    "        _ = []\n",
    "\n",
    "        for elem in search_query['include']:\n",
    "            _.append(\"((\" + \") OR (\".join(elem) + \"))\")\n",
    "        search_text = \"((\" + \") AND (\".join(_) + \"))\"\n",
    "\n",
    "        if 'not_include' in search_query.keys():\n",
    "            _ = []\n",
    "            for elem in search_query['not_include']:\n",
    "                _.append(\"((\" + \") AND (\".join(elem) + \"))\")\n",
    "            search_text += \" -((\" + \") OR (\".join(_) + \"))\"\n",
    "\n",
    "        if 'from' in search_query.keys():\n",
    "            search_text += ' (from:' + ' OR from:'.join(search_query['from']) + ')'\n",
    "\n",
    "        if 'to' in search_query.keys():\n",
    "            search_text += ' (to:' + ' OR to:'.join(search_query['to']) + ')'\n",
    "\n",
    "        if 'lang' in search_query.keys():\n",
    "            search_text += ' lang:' + search_query['lang']\n",
    "\n",
    "        if 'since' in search_query.keys():\n",
    "            search_text += f\" since:{search_query['since']}\"\n",
    "\n",
    "        if 'until' in search_query.keys():\n",
    "            search_text += f\" until:{search_query['until']}\"\n",
    "\n",
    "        if 'mentioning' in search_query.keys():\n",
    "            search_text += ' (@' + ' @'.join([search_query['mentioning']]) + ')'\n",
    "            \n",
    "        return search_text\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_tweet_data(self, exprot_file_name, search_query, max_tweet=None):\n",
    "        \n",
    "        ''' \n",
    "        *** Extract tweets ***\n",
    "        :param search_query: (dict or path)\n",
    "            - If search_query is a path, it should redirect to a .txt file that contains search query you want\n",
    "            - If search_query is a dict. it should defined like 'create_search_query' method.\n",
    "        \n",
    "        :param max_tweet: (int) Ahen reachs to the number, running will stop\n",
    "        '''\n",
    "        \n",
    "        sleep(1)\n",
    "        self.driver.get('https://twitter.com/explore')\n",
    "        sleep(4)\n",
    "        \n",
    "        \n",
    "        # Read Unique IDs\n",
    "        unique_tweet_ids = set()\n",
    "        if os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "            df = pd.read_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\")\n",
    "            unique_tweet_ids = set(df.Tweet_ID)\n",
    "        \n",
    "        \n",
    "        # Create Search Query\n",
    "        search_list = []\n",
    "        if isinstance(search_query, dict):\n",
    "            search_list.append(self.create_search_query(search_query))\n",
    "        else:\n",
    "            with open(search_query) as fp:\n",
    "                search_list = fp.read().split('\\n')\n",
    "        \n",
    "        progress = 1\n",
    "        for exp_ind, search_text in enumerate(search_list):\n",
    "            # Enter Search Query in Twitter search box\n",
    "            sleep(4)\n",
    "            search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "            search.send_keys(Keys.CONTROL + \"a\")\n",
    "            search.send_keys(Keys.DELETE)\n",
    "            search.send_keys(search_text)\n",
    "            search.send_keys(Keys.RETURN)\n",
    "\n",
    "            # Go to 'Latest' tweets\n",
    "            sleep(10)\n",
    "            self.driver.find_element('link text', 'Latest').click()\n",
    "\n",
    "            # Scraping\n",
    "            scrolling = True\n",
    "            list_ = []\n",
    "            j = 1\n",
    "\n",
    "            while scrolling:\n",
    "                sleep(2)\n",
    "                tweets = self.driver.find_elements('xpath', '//article[@data-testid=\"tweet\"]')\n",
    "                sleep(1)\n",
    "                \n",
    "                if progress >= 500 * j:\n",
    "                    self.driver.close()\n",
    "                    sleep(15)\n",
    "                    options = EdgeOptions()\n",
    "                    options.use_chromium = True\n",
    "                    options.add_argument(\"-inprivate\")\n",
    "                    options.binary_location = r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "                    self.driver = Edge('/mnt/f/msedgedriver.exe', options=options)\n",
    "                    sleep(3)\n",
    "\n",
    "                    self.driver.get('https://twitter.com/explore')\n",
    "                    sleep(4)\n",
    "                    search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "                    search.send_keys(Keys.CONTROL + \"a\")\n",
    "                    search.send_keys(Keys.DELETE)\n",
    "                    if j > 1:\n",
    "                        search_text = search_text[:-17]\n",
    "                    search.send_keys(search_text + ' until:' + str(datetime)[:10])\n",
    "                    search.send_keys(Keys.RETURN)\n",
    "                    sleep(6)\n",
    "                    self.driver.find_element('link text', 'Latest').click()\n",
    "                    sleep(2)\n",
    "                    \n",
    "                    j += 1\n",
    "                \n",
    "                for tweet in tweets[-min(24, len(tweets)-1):]:\n",
    "                    try:\n",
    "                        reply_to, hashtag = str(), str()\n",
    "                        text = tweet.find_element('xpath', './/div[@data-testid=\"tweetText\"]').text\n",
    "\n",
    "                        if 'Replying to \\n@' in tweet.text:\n",
    "                            reply_to = tweet.find_element(\n",
    "                                'xpath', './/div[@class=\"css-901oao r-1bwzh9t r-37j5jr r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"]'\n",
    "                            ).text.split()\n",
    "                            reply_to = [id_ for id_ in reply_to if id_[0] == '@']\n",
    "\n",
    "                        if '#' in text:\n",
    "                            words = text.split()\n",
    "                            hashtag = [word for word in words if word[0] == '#']\n",
    "\n",
    "                        reply = tweet.find_element('xpath', './/div[@data-testid=\"reply\"]').text\n",
    "                        retweet = tweet.find_element('xpath', './/div[@data-testid=\"retweet\"]').text\n",
    "                        like = tweet.find_element('xpath', './/div[@data-testid=\"like\"]').text\n",
    "\n",
    "                        header = tweet.find_element('xpath', './/div[@data-testid=\"User-Names\"]')\n",
    "                        _ = header.find_element('xpath', './/time').get_attribute('datetime')\n",
    "                        datetime = dt.datetime.strptime(_, '%Y-%m-%dT%H:%M:%S.000Z') + timedelta(hours=4.5)\n",
    "\n",
    "                        info = header.text.split('@')\n",
    "                        username = info[0].strip('\\n')\n",
    "                        user_id = '@' + info[1].split()[0]\n",
    "\n",
    "                        tweet_id = sha256(\"\".join([user_id, str(datetime)]).encode()).hexdigest()\n",
    "\n",
    "\n",
    "                        if tweet_id not in unique_tweet_ids:\n",
    "                            sys.stdout.write('\\r')\n",
    "                            sys.stdout.write(f\"Saved: {progress - progress % 100}       \" +\n",
    "                                             f\"Current_State: [ Search_Query: {exp_ind + 1} from {len(search_list)}    \" +\n",
    "                                             f\"Tweet: {progress}    \" +\n",
    "                                             f\"Date: {str(datetime)[:7]} ] {' '*20}‌\")\n",
    "                            \n",
    "                            sys.stdout.flush()\n",
    "                            progress += 1\n",
    "\n",
    "                            list_.append([exp_ind, tweet_id, username, user_id, datetime, text, reply_to, hashtag, reply, retweet, like])\n",
    "                            unique_tweet_ids.add(tweet_id)\n",
    "\n",
    "                            if len(list_) >= 100:\n",
    "                                self.save_file(list_, exprot_file_name)\n",
    "                                list_ = []\n",
    "\n",
    "                            if max_tweet:\n",
    "                                if progress > max_tweet:\n",
    "                                    self.save_file(list_, exprot_file_name)\n",
    "                                    list_ = []\n",
    "                                    scrolling = False\n",
    "                                    break\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Scrolling\n",
    "                scroll_attemp = 0\n",
    "                scroll_up = 0\n",
    "                \n",
    "                last_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                while True:\n",
    "                    self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                    sleep(4)\n",
    "                    current_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                    if current_positon == last_positon:\n",
    "                        scroll_attemp += 1\n",
    "                        \n",
    "                        if scroll_attemp == 2 and scroll_up <= 2:\n",
    "                            self.driver.execute_script(f\"window.scrollTo(0, {self.driver.execute_script('return window.pageYOffset;')-1800})\")\n",
    "                            sleep(3)\n",
    "                            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                            sleep(4)\n",
    "                        \n",
    "                        if scroll_attemp >= 5:\n",
    "                            if len(list_) > 0:\n",
    "                                self.save_file(list_, exprot_file_name)\n",
    "                                list_ = []\n",
    "\n",
    "                            scrolling = False\n",
    "                            break\n",
    "                        else:\n",
    "                            sleep(5)\n",
    "                    else:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de12032-6a9d-4503-8325-d7e4b8ea56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gethi_Chart:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def replace_compound_words(self, input_, compound_words_path, mode=1):\n",
    "        with open(compound_words_path) as fp:\n",
    "            compound_words = fp.read()\n",
    "        \n",
    "        compound_words = compound_words.split('\\n')\n",
    "        compound_words = {item:'_'.join(item.split()) for item in compound_words}\n",
    "        \n",
    "        if mode == 1:\n",
    "            for key, value in compound_words.items():\n",
    "                input_ = input_.str.replace(key, value)\n",
    "            \n",
    "        else:\n",
    "            for key, value in compound_words.items():\n",
    "                input_ = input_.str.replace(value, key)\n",
    "                \n",
    "        return input_\n",
    "    \n",
    "    \n",
    "    def create_node_edge(self, tweet_data_dir, theme_dir, output_dir):\n",
    "        df = pd.read_csv(tweet_data_dir)\n",
    "        df = df.drop_duplicates(subset = \"Tweet_ID\")\n",
    "        tweet_texts = ' Oadf6dsaf8dfa '.join(list(df.Text))\n",
    "        tweet_texts2 = copy.deepcopy(tweet_texts)\n",
    "\n",
    "        _ = pd.read_csv(theme_dir)\n",
    "        themes = {_.iloc[i, 0]:[sha256(_.iloc[i, 1].encode()).hexdigest()[:18], _.iloc[i, 1].replace(\" \", \"_\")] for i in range(len(_))}\n",
    "\n",
    "        \n",
    "        # Detect Nodes\n",
    "        keys = list(themes.keys())\n",
    "        keys = sorted(keys, key=len, reverse=True)\n",
    "\n",
    "        for item in keys:\n",
    "            tweet_texts = tweet_texts.replace(item, themes[item][0])\n",
    "\n",
    "        for theme_ in themes.values():\n",
    "            tweet_texts = tweet_texts.replace(theme_[0], theme_[1])\n",
    "\n",
    "        filt = [*[item[1] for item in list(themes.values())], 'Oadf6dsaf8dfa']\n",
    "\n",
    "        _ = list(filter(lambda x: x in filt, tweet_texts.split()))\n",
    "        tweet_list = [list(group) for k, group in groupby(_, lambda x: x == \"Oadf6dsaf8dfa\") if not k]\n",
    "        \n",
    "        _ = [(item[0], item[0].replace(\"_\", \" \"), item[1]) for item in Counter(_).most_common()[1:]]\n",
    "        df_output_nodes = pd.DataFrame(_, columns=['Id', 'Label', 'Weight'])\n",
    "        df_output_nodes.to_csv(output_dir / 'nodes.csv', index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        \n",
    "        # Detect Edges\n",
    "        edges = defaultdict(int)\n",
    "        for tweet in tweet_list:\n",
    "            set_ = set()\n",
    "            if len(tweet) <=1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(tweet)):\n",
    "                for j in range(i+1, len(tweet)):\n",
    "                    if tweet[i] != tweet[j]:\n",
    "                        if (tweet[i], tweet[j]) in set_:\n",
    "                            continue\n",
    "                        else:\n",
    "                            edges[(tweet[i], tweet[j])] += 1\n",
    "                            set_.add((tweet[i], tweet[j]))\n",
    "            \n",
    "        tweet_list = []\n",
    "        tweet_list_word = []\n",
    "\n",
    "        for key, value in edges.items():\n",
    "            tweet_list.append((*key, value))\n",
    "            \n",
    "        df_output_edges = pd.DataFrame(tweet_list, columns=['Source', 'Target', 'Weight'])\n",
    "        df_output_edges.to_csv(output_dir / 'edges.csv', index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "    \n",
    "    def remove_stopwords(self, text: str, stopwords: list):\n",
    "        \"\"\"\n",
    "        :param text: text you want to delete stopwords from dat\n",
    "        :param stopwords: list of stopwords\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = filter(lambda word: word not in stopwords, tokens)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "    def most_common_words(self, tweet_data_dir, stopwords_dir, output_dir):\n",
    "        df = pd.read_csv(tweet_data_dir)\n",
    "        df = df.drop_duplicates(subset = \"Tweet_ID\")\n",
    "        \n",
    "        df.Text = self.replace_compound_words(df.Text, DATA_DIR / 'data/Input/compound_words.txt')\n",
    "        \n",
    "        with open(stopwords_dir) as fp:\n",
    "            stopwords = fp.read()\n",
    "        stopwords = stopwords.split('\\n')\n",
    "        \n",
    "        list_ = list(df.Text)\n",
    "        list_ = [str(text) for text in list_]\n",
    "        text = ' '.join(list_)\n",
    "        text = self.remove_stopwords(text, stopwords)\n",
    "        text = text.split()\n",
    "        df = pd.DataFrame(Counter(text).most_common(), columns=['word', 'count'])\n",
    "        df.word = self.replace_compound_words(df.word, DATA_DIR / 'data/Input/compound_words.txt', mode=2)\n",
    "        df.to_csv(output_dir / 'most_common_words.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1e0e-cfa9-430b-a2fd-cede581c0193",
   "metadata": {},
   "source": [
    "### \n",
    "## Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "366307f5-e692-4399-a694-b32f517725a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Service.__del__ at 0x7fcc1d273280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 171, in __del__\n",
      "    self.stop()\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 146, in stop\n",
      "    self.send_remote_shutdown_command()\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/site-packages/selenium/webdriver/common/service.py\", line 122, in send_remote_shutdown_command\n",
      "    request.urlopen(\"%s/shutdown\" % self.service_url)\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 222, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 542, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 502, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 1383, in http_open\n",
      "    return self.do_open(http.client.HTTPConnection, req)\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/urllib/request.py\", line 1358, in do_open\n",
      "    r = h.getresponse()\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/kourosh/anaconda3/envs/py38/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "options.add_argument(\"-inprivate\")\n",
    "options.binary_location = r\"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\\msedge.exe\"\n",
    "driver = Edge('/mnt/f/msedgedriver.exe', options=options)\n",
    "\n",
    "tweet = Twitter_scraper(driver)\n",
    "\n",
    "\n",
    "tweet.get_tweet_data(\n",
    "    search_query={'include': [[\n",
    "                               'اومارکت',\n",
    "                               'اوکالا',\n",
    "                              ],                             \n",
    "                             ],\n",
    "\n",
    "                  'lang': 'fa',\n",
    "                 },\n",
    "\n",
    "    max_tweet=None,\n",
    "    exprot_file_name='Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74b5f8-4262-4342-bf06-8d9be32245ad",
   "metadata": {},
   "source": [
    "### \n",
    "## Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e91625-cc49-48ff-8404-a0a7baced436",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = Gethi_Chart()\n",
    "\n",
    "chart.most_common_words(tweet_data_dir = DATA_DIR / 'data/Dataset.csv',\n",
    "                        stopwords_dir = DATA_DIR / 'data/Input/persian_stop_words.txt',\n",
    "                        output_dir = DATA_DIR / 'data/Themes'\n",
    "                       )\n",
    "\n",
    "chart.create_node_edge(tweet_data_dir = DATA_DIR / 'data/Dataset.csv',\n",
    "                       theme_dir = DATA_DIR / 'data/Themes/Themes.csv',\n",
    "                       output_dir = DATA_DIR / 'data/Gephi',\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a388c-071b-4b89-ac05-08131cb0a264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
