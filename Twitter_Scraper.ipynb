{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9a68ed-60fb-452b-b0ac-44f84697c537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from hashlib import sha256\n",
    "import os.path\n",
    "from src import DATA_DIR\n",
    "from hazm import Normalizer, sent_tokenize, word_tokenize\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1771c969-1f18-4a73-981f-9813f3f78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twitter_scraper:\n",
    "    def __init__(self, driver):\n",
    "        self.driver = driver\n",
    "        \n",
    "\n",
    "    def save_file(self, list_, exprot_file_name):\n",
    "        df = pd.DataFrame(list_, columns=['Search_Query', 'Tweet_ID',\n",
    "                                          'Username', 'User_ID',\n",
    "                                          'DateTime', 'Text',\n",
    "                                          'Reply_to', 'Hashtag',\n",
    "                                          'Reply', 'Retweet',\n",
    "                                          'Like',\n",
    "                                         ])\n",
    "\n",
    "        if not os.path.isfile(DATA_DIR / f'data/{exprot_file_name}.csv'):\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "        else:\n",
    "            df.to_csv(DATA_DIR / f'data/{exprot_file_name}.csv', encoding=\"utf-8-sig\", mode='a', header=False, index=False)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    def create_search_query(self, search_query, exclude_replies=False):\n",
    "        \n",
    "        '''\n",
    "        *** Create Search Query ***\n",
    "        :param search_query: (dict or path) Gets search query. If search_querys type is dictionary, It can have following keys:\n",
    "            - include: (list) Words that must be in tweet text. Each element is a list. This method will combine different elements with 'AND'\n",
    "              operator and each element words with 'OR' operator.\n",
    "              \n",
    "            - not_include: (list) Words that shouldn't be in tweet text. Each element is a list. This method will combine different elements\n",
    "              with 'AND' operator and each element words with 'OR' operator.\n",
    "\n",
    "            - from: (list) Filters tweets that sent from specific Twitter accounts\n",
    "\n",
    "            - to: (list) Filters tweets that sent in reply to specific Twitter accounts\n",
    "\n",
    "            - since: (str, samp = “2015-12-21”) Containing tweets that sent since a specific date\n",
    "\n",
    "            - until: (str, samp = “2022-12-21”) Containing tweets that sent until a specific date\n",
    "\n",
    "            - mentioning: (list) Containing tweets that mentioning specific Twitter accounts\n",
    "\n",
    "            - question: ('True', 'False') If True, gets all tweet, that were recognized by twitter AI as a question\n",
    "                \n",
    "        '''\n",
    "\n",
    "        search_text = ''\n",
    "        \n",
    "        if 'include' in search_query.keys():\n",
    "            _ = []\n",
    "            for elem in search_query['include']:\n",
    "                if len(elem) == 0:\n",
    "                    pass\n",
    "\n",
    "                elif len(elem) > 1:\n",
    "                    _.append(\"((\" + \") AND (\".join(elem) + \"))\")\n",
    "\n",
    "                else:\n",
    "                    _.append(\"(\" + str(elem[0]) + \")\")\n",
    "\n",
    "            if len(search_query['include']) > 1:\n",
    "                search_text += \"(\" + \" OR \".join(_) + \")\"\n",
    "\n",
    "            else:\n",
    "                search_text += _[0]\n",
    "\n",
    "        if 'not_include' in search_query.keys():\n",
    "            _ = []\n",
    "            for elem in search_query['not_include']:\n",
    "                if len(elem) == 0:\n",
    "                    pass\n",
    "\n",
    "                elif len(elem) > 1:\n",
    "                    _.append(\"((\" + \") AND (\".join(elem) + \"))\")\n",
    "\n",
    "                else:\n",
    "                    _.append(\"(\" + str(elem[0]) + \")\")\n",
    "\n",
    "            if len(search_query['not_include']) > 1:\n",
    "                search_text += \" -(\" + \" OR \".join(_) + \")\"\n",
    "\n",
    "            else:\n",
    "                search_text += f\"-{_[0]}\"\n",
    "\n",
    "\n",
    "\n",
    "        if 'from' in search_query.keys():\n",
    "            search_text += ' (from:' + ' OR from:'.join(search_query['from']) + ')'\n",
    "\n",
    "        if 'to' in search_query.keys():\n",
    "            search_text += ' (to:' + ' OR to:'.join(search_query['to']) + ')'\n",
    "\n",
    "        if 'lang' in search_query.keys():\n",
    "            search_text += ' lang:' + search_query['lang']\n",
    "\n",
    "        if 'since' in search_query.keys():\n",
    "            search_text += f\" since:{search_query['since']}\"\n",
    "\n",
    "        if 'until' in search_query.keys():\n",
    "            search_text += f\" until:{search_query['until']}\"\n",
    "\n",
    "        if 'mentioning' in search_query.keys():\n",
    "            search_text += ' (@' + ' @'.join([search_query['mentioning']]) + ')'\n",
    "\n",
    "        if exclude_replies:\n",
    "            search_text += ' exclude:replies'\n",
    "            \n",
    "        return search_text\n",
    "\n",
    "\n",
    "\n",
    "    def get_tweet_data(self, exprot_file_name, search_query, exclude_replies=False, max_tweet=None, error=0):\n",
    "        # try:\n",
    "            ''' \n",
    "            *** Extract tweets ***\n",
    "            :param search_query: (dict or path)\n",
    "                - If search_query is a path, it should redirect to a .txt file that contains search query you want\n",
    "                - If search_query is a dict. it should defined like 'create_search_query' method.\n",
    "            \n",
    "            :param max_tweet: (int) Ahen reachs to the number, running will stop\n",
    "            '''\n",
    "\n",
    "            unique_tweet_ids = set()\n",
    "            \n",
    "            sleep(1)\n",
    "            self.driver.get('https://twitter.com/explore')\n",
    "            sleep(4)\n",
    "            \n",
    "            \n",
    "            # Create Search Query\n",
    "            search_list = []\n",
    "            if isinstance(search_query, dict):\n",
    "                search_list.append(self.create_search_query(search_query, exclude_replies))\n",
    "            else:\n",
    "                with open(search_query) as fp:\n",
    "                    search_list = fp.read().split('\\n')\n",
    "            \n",
    "            progress = 1\n",
    "            for exp_ind, search_text in enumerate(search_list):\n",
    "                # Enter Search Query in Twitter search box\n",
    "                search = self.driver.find_element('xpath', '//label[@data-testid=\"SearchBox_Search_Input_label\"]')\n",
    "                search.send_keys(Keys.CONTROL + \"a\")\n",
    "                search.send_keys(Keys.DELETE)\n",
    "                search.send_keys(search_text)\n",
    "                search.send_keys(Keys.RETURN)\n",
    "\n",
    "                # Go to 'Latest' tweets\n",
    "                sleep(2)\n",
    "                self.driver.find_element('link text', 'Latest').click()\n",
    "\n",
    "                # Scraping\n",
    "                scrolling = True\n",
    "                list_ = []\n",
    "                j = 1\n",
    "                k = 0\n",
    "                flag = 0\n",
    "\n",
    "                self.driver.execute_script(\"document.body.style.zoom='50%'\")\n",
    "                sleep(2)\n",
    "\n",
    "                while scrolling:\n",
    "                    sleep(0.5)\n",
    "                    tweets = self.driver.find_elements('xpath', '//article[@data-testid=\"tweet\"]')\n",
    "                    sleep(0.2)\n",
    "                    k += 1\n",
    "\n",
    "                    for tweet in tweets[-min(10, len(tweets)):]:\n",
    "                        header = tweet.find_element('xpath', './/div[@data-testid=\"User-Names\"]')\n",
    "                        _ = header.find_element('xpath', './/time').get_attribute('datetime')\n",
    "                        datetime = dt.datetime.strptime(_, '%Y-%m-%dT%H:%M:%S.000Z') + timedelta(hours=4.5)\n",
    "\n",
    "                        info = header.find_elements('xpath', './/div[@class=\"css-1dbjc4n r-1wbh5a2 r-dnmrzs\"]')\n",
    "                        username = info[0].text\n",
    "                        user_id = info[1].text\n",
    "\n",
    "                        if not len(user_id):\n",
    "                            continue\n",
    "\n",
    "                        tweet_id = sha256(\"\".join([user_id, str(datetime)]).encode()).hexdigest()\n",
    "\n",
    "                        if tweet_id in unique_tweet_ids:\n",
    "                            continue\n",
    "                        \n",
    "                        unique_tweet_ids.add(tweet_id)\n",
    "\n",
    "\n",
    "                        reply_to, hashtag = str(), str()\n",
    "                        text = tweet.find_element('xpath', './/div[@data-testid=\"tweetText\"]').text\n",
    "\n",
    "                        reply_to = ''\n",
    "                        if 'Replying to \\n@' in tweet.text:\n",
    "                            _ = tweet.find_elements('xpath', './/div[@class=\"css-901oao r-1bwzh9t r-37j5jr r-a023e6 r-16dba41 r-rjixqe r-bcqeeo r-qvutc0\"]')\n",
    "                            \n",
    "                            if len(_):\n",
    "                                _ = _[0].text.split()\n",
    "                                reply_to = [id_ for id_ in _ if id_[0] == '@']\n",
    "\n",
    "                        if '#' in text:\n",
    "                            words = text.split()\n",
    "                            hashtag = [word for word in words if word[0] == '#']\n",
    "\n",
    "                        reply = tweet.find_element('xpath', './/div[@data-testid=\"reply\"]').text\n",
    "                        retweet = tweet.find_element('xpath', './/div[@data-testid=\"retweet\"]').text\n",
    "                        like = tweet.find_element('xpath', './/div[@data-testid=\"like\"]').text\n",
    "\n",
    "\n",
    "                        sys.stdout.write('\\r')\n",
    "                        sys.stdout.write(f\"Error: {error}    \" +\n",
    "                                         f\"File: {exprot_file_name[-24:-4]}    \" + \n",
    "                                         f\"Tweet: {progress}    \" +\n",
    "                                         f\"Date: {str(datetime)} {' '*10}‌\")\n",
    "                        \n",
    "                        sys.stdout.flush()\n",
    "                        progress += 1\n",
    "\n",
    "                        list_.append([exp_ind, tweet_id, username, user_id, datetime, text, reply_to, hashtag, reply, retweet, like])\n",
    "\n",
    "                        if len(list_) >= 100:\n",
    "                            self.save_file(list_, exprot_file_name)\n",
    "                            list_ = []\n",
    "\n",
    "                        if max_tweet:\n",
    "                            if progress > max_tweet:\n",
    "                                self.save_file(list_, exprot_file_name)\n",
    "                                list_ = []\n",
    "                                scrolling = False\n",
    "                                break\n",
    "\n",
    "\n",
    "                    # Scrolling\n",
    "                    scroll_attemp = 0\n",
    "                    \n",
    "                    last_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                    while True:\n",
    "                        self.driver.execute_script(f\"window.scrollTo(0, {self.driver.execute_script('return window.pageYOffset;')+450})\")\n",
    "                        sleep(1)\n",
    "                        current_positon = self.driver.execute_script('return window.pageYOffset;')\n",
    "\n",
    "                        if current_positon == last_positon:\n",
    "                            scroll_attemp += 1\n",
    "                            \n",
    "                            if scroll_attemp == 2:\n",
    "                                self.driver.execute_script(f\"window.scrollTo(0, {self.driver.execute_script('return window.pageYOffset;')-1800})\")\n",
    "                                sleep(2)\n",
    "                                self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "                                sleep(2)\n",
    "                            \n",
    "                            if scroll_attemp >= 3:\n",
    "                                if len(list_) > 0:\n",
    "                                    self.save_file(list_, exprot_file_name)\n",
    "                                    list_ = []\n",
    "\n",
    "                                scrolling = False\n",
    "                                break\n",
    "                            else:\n",
    "                                sleep(2)\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            self.driver.close()\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de12032-6a9d-4503-8325-d7e4b8ea56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gephi_Chart:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def replace_compound_words(self, input_, compound_words_path, mode=1):\n",
    "        with open(compound_words_path) as fp:\n",
    "            compound_words = fp.read()\n",
    "        \n",
    "        compound_words = compound_words.split('\\n')\n",
    "        compound_words = {item:'_'.join(item.split()) for item in compound_words}\n",
    "        \n",
    "        if mode == 1:\n",
    "            for key, value in compound_words.items():\n",
    "                input_ = input_.str.replace(key, value)\n",
    "            \n",
    "        else:\n",
    "            for key, value in compound_words.items():\n",
    "                input_ = input_.str.replace(value, key)\n",
    "                \n",
    "        return input_\n",
    "\n",
    "\n",
    "    def create_node_edge(self, tweet_data_dir, theme_dir, output_dir, file_name_, remove_word):\n",
    "        df = pd.read_csv(tweet_data_dir)\n",
    "        # df = df.drop_duplicates(subset=\"Tweet_ID\")\n",
    "\n",
    "        with open(remove_word) as fp:\n",
    "            remove_ = fp.read()\n",
    "        remove_ = remove_.split('\\n')\n",
    "        \n",
    "        for word in remove_:\n",
    "            df = df[~df['Text'].str.contains(word)]\n",
    "\n",
    "\n",
    "        tweet_texts = ' Oadf#dsaf#dfa '.join(list(df.Text))\n",
    "        tweet_texts2 = copy.deepcopy(tweet_texts)\n",
    "\n",
    "        _ = pd.read_csv(theme_dir)\n",
    "        themes = {_.iloc[i, 0]:[sha256(_.iloc[i, 1].encode()).hexdigest()[:18], _.iloc[i, 1].replace(\" \", \"_\")] for i in range(len(_))}\n",
    "        class_ = {_.iloc[i, 1]:_.iloc[i, 2] for i in range(len(_))}\n",
    "\n",
    "        \n",
    "        # Detect Nodes\n",
    "        keys = list(themes.keys())\n",
    "        keys = sorted(keys, key=len, reverse=True)\n",
    "\n",
    "        for item in keys:\n",
    "            tweet_texts = tweet_texts.replace(item, themes[item][0])\n",
    "\n",
    "        for theme_ in themes.values():\n",
    "            tweet_texts = tweet_texts.replace(theme_[0], theme_[1])\n",
    "\n",
    "        filt = [*[item[1] for item in list(themes.values())], 'Oadf#dsaf#dfa']\n",
    "\n",
    "        _ = list(filter(lambda x: x in filt, tweet_texts.split()))\n",
    "        tweet_list = [list(group) for k, group in groupby(_, lambda x: x == \"Oadf#dsaf#dfa\") if not k]\n",
    "\n",
    "        _ = [(item[0], item[0].replace(\"_\", \" \"), item[1], class_[item[0].replace(\"_\", \" \")]) for item in Counter(_).most_common() if item[0] != \"Oadf#dsaf#dfa\"]\n",
    "        df_output_nodes = pd.DataFrame(_, columns=['Id', 'Label', 'Weight', 'Class'])\n",
    "        df_output_nodes.to_csv(output_dir / f'{file_name_}_nodes.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "        # Detect Edges\n",
    "        edges = defaultdict(int)\n",
    "\n",
    "        event_log = []\n",
    "        \n",
    "        for ind, tweet in enumerate(tweet_list):\n",
    "            if len(tweet) >= 4:\n",
    "                for theme in tweet:\n",
    "                    event_log.append([ind, theme])\n",
    "                    \n",
    "        df_event_log = pd.DataFrame(event_log, columns=['Id', 'Theme'])\n",
    "        df_event_log.to_csv(output_dir / 'event_log.csv', index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        \n",
    "        for tweet in tweet_list:\n",
    "            set_ = set()\n",
    "            if len(tweet) <=1:\n",
    "                continue\n",
    "\n",
    "            for i in range(len(tweet)):\n",
    "                for j in range(i+1, len(tweet)):\n",
    "                    if tweet[i] != tweet[j]:\n",
    "                        if (tweet[i], tweet[j]) in set_:\n",
    "                            continue\n",
    "                        else:\n",
    "                            edges[(tweet[i], tweet[j])] += 1\n",
    "                            set_.add((tweet[i], tweet[j]))\n",
    "\n",
    "        tweet_list = []\n",
    "        tweet_list_word = []\n",
    "\n",
    "        for key, value in edges.items():\n",
    "            tweet_list.append((*key, value))\n",
    "\n",
    "        df_output_edges = pd.DataFrame(tweet_list, columns=['Source', 'Target', 'Weight'])\n",
    "        df_output_edges.to_csv(output_dir / f'{file_name_}_edges.csv', index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "    \n",
    "    def remove_stopwords(self, text: str, stopwords: list):\n",
    "        \"\"\"\n",
    "        :param text: text you want to delete stopwords from dat\n",
    "        :param stopwords: list of stopwords\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = filter(lambda word: word not in stopwords, tokens)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "    def most_common_words(self, tweet_data_dir, stopwords_dir, output_dir, remove_word):\n",
    "        df = pd.read_csv(tweet_data_dir)\n",
    "        df = df.drop_duplicates(subset = \"Tweet_ID\")\n",
    "        \n",
    "        df.Text = self.replace_compound_words(df.Text, DATA_DIR / 'data/Input/compound_words.txt')\n",
    "        \n",
    "        with open(remove_word) as fp:\n",
    "            remove_ = fp.read()\n",
    "        remove_ = remove_.split('\\n')\n",
    "        \n",
    "        for word in remove_:\n",
    "            df = df[~df['Text'].str.contains(word)]\n",
    "        \n",
    "        with open(stopwords_dir) as fp:\n",
    "            stopwords = fp.read()\n",
    "        stopwords = stopwords.split('\\n')\n",
    "        \n",
    "        list_ = list(df.Text)\n",
    "        list_ = [str(text) for text in list_]\n",
    "        text = ' '.join(list_)\n",
    "        text = self.remove_stopwords(text, stopwords)\n",
    "        text = text.split()\n",
    "        df = pd.DataFrame(Counter(text).most_common(), columns=['word', 'count'])\n",
    "        df.word = self.replace_compound_words(df.word, DATA_DIR / 'data/Input/compound_words.txt', mode=2)\n",
    "        df.to_csv(output_dir, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa1e0e-cfa9-430b-a2fd-cede581c0193",
   "metadata": {},
   "source": [
    "### \n",
    "## Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "366307f5-e692-4399-a694-b32f517725a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "options.add_argument(\"-inprivate\")\n",
    "driver = Edge(DATA_DIR / 'msedgedriver.exe', options=options)\n",
    "\n",
    "tweet = Twitter_scraper(driver)\n",
    "\n",
    "\n",
    "tweet.get_tweet_data(\n",
    "    search_query={\n",
    "        'include': [\n",
    "            ['مهرگیاه', 'دمنوش'],\n",
    "            ['نیوشا' , 'دمنوش'],\n",
    "        ],\n",
    "        \n",
    "        'lang': 'fa',\n",
    "        'not_from': ['jahan_n_'],\n",
    "        'since': '2020-01-01',\n",
    "        'until': '2022-05-01'\n",
    "    },\n",
    "\n",
    "    max_tweet=None,\n",
    "    exprot_file_name='Herbal_Tea')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74b5f8-4262-4342-bf06-8d9be32245ad",
   "metadata": {},
   "source": [
    "### \n",
    "## Output Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e91625-cc49-48ff-8404-a0a7baced436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Common Words\n",
    "\n",
    "chart = Gethi_Chart()\n",
    "\n",
    "chart.most_common_words(tweet_data_dir = DATA_DIR / 'data/Kitchen.csv',\n",
    "                        stopwords_dir = DATA_DIR / 'data/Input/persian_stop_words.txt',\n",
    "                        output_dir = DATA_DIR / 'data/Themes/Kitchen_Most_common.csv',\n",
    "                        remove_word = DATA_DIR / 'data/Remove.txt'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4729c996-38fd-4b57-9704-5d8c80f941c2",
   "metadata": {},
   "source": [
    "##### Node And Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c64a388c-071b-4b89-ac05-08131cb0a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = Gephi_Chart()\n",
    "\n",
    "chart.create_node_edge(tweet_data_dir = DATA_DIR / 'data/Gephi/Pack/Tornado Pos.csv',\n",
    "                       theme_dir = DATA_DIR / 'data/Gephi/Pack/Tornado.csv',\n",
    "                       output_dir = DATA_DIR / 'data/Gephi/Pack/',\n",
    "                       file_name_ = 'Tornado_Pos',\n",
    "                       remove_word = DATA_DIR / 'data/Remove.txt',\n",
    "                       \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27094bc-9c59-45cc-95f9-d55596a3b5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
